{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenizes a given text into individual words.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Load dataset (Replace 'your_dataset.csv' with your actual file)\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "\n",
        "# Assume the dataset has a column named 'review'\n",
        "df['tokenized_review'] = df['review'].apply(tokenize_text)\n",
        "\n",
        "# Display tokenized data\n",
        "print(df[['review', 'tokenized_review']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKEIHDEg1nZu",
        "outputId": "8bccc038-6dfb-4350-fcd8-94fc26278f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review  \\\n",
            "0  One of the other reviewers has mentioned that ...   \n",
            "1  A wonderful little production. <br /><br />The...   \n",
            "2  I thought this was a wonderful way to spend ti...   \n",
            "3  Basically there's a family where a little boy ...   \n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
            "\n",
            "                                    tokenized_review  \n",
            "0  [One, of, the, other, reviewers, has, mentione...  \n",
            "1  [A, wonderful, little, production, ., <, br, /...  \n",
            "2  [I, thought, this, was, a, wonderful, way, to,...  \n",
            "3  [Basically, there, 's, a, family, where, a, li...  \n",
            "4  [Petter, Mattei, 's, ``, Love, in, the, Time, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Removes stopwords from a tokenized list of words.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"Performs lemmatization on a tokenized list of words.\"\"\"\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def tokens_to_text(tokens):\n",
        "    \"\"\"Converts tokenized words back into a single text string.\"\"\"\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Ensure required columns exist\n",
        "if 'tokenized_review' not in df.columns or 'sentiment' not in df.columns:\n",
        "    raise ValueError(\"Dataset must contain 'tokenized_review' and 'sentiment' columns.\")\n",
        "\n",
        "# Preprocessing: Stopword removal, Lemmatization, and Text Formatting\n",
        "df['filtered_review'] = df['tokenized_review'].apply(remove_stopwords)\n",
        "df['lemmatized_review'] = df['filtered_review'].apply(lemmatize_tokens)\n",
        "df['processed_text'] = df['lemmatized_review'].apply(tokens_to_text)\n",
        "\n",
        "# Splitting dataset into train & test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "### MODEL TRAINING ###\n",
        "\n",
        "# Logistic Regression Classifier\n",
        "pipeline_lr = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
        "pipeline_lr.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline_lr.predict(X_test)\n",
        "\n",
        "# Naive Bayes Classifier\n",
        "pipeline_nb = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "pipeline_nb.fit(X_train, y_train)\n",
        "y_pred_nb = pipeline_nb.predict(X_test)\n",
        "\n",
        "### MODEL EVALUATION ###\n",
        "print(\"\\n=== Logistic Regression Model ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "print(\"\\n=== Naive Bayes Model ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "### PREDICT SENTIMENT FOR WHOLE DATASET ###\n",
        "df['predicted_sentiment'] = pipeline_lr.predict(df['processed_text'])\n",
        "\n",
        "# Display sample predictions\n",
        "print(df[['processed_text', 'sentiment', 'predicted_sentiment']].head())\n",
        "\n",
        "# Save predictions to CSV\n",
        "df.to_csv(\"sentiment_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to 'sentiment_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8IMEykk2OvQ",
        "outputId": "3a896af2-5245-4de2-aadd-31db400093a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression Model ===\n",
            "Accuracy: 0.8964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.88      0.89      4961\n",
            "    positive       0.89      0.91      0.90      5039\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "\n",
            "=== Naive Bayes Model ===\n",
            "Accuracy: 0.8665\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.88      0.87      4961\n",
            "    positive       0.88      0.85      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "                                      processed_text sentiment  \\\n",
            "0  One reviewer mentioned watching 1 Oz episode '...  positive   \n",
            "1  wonderful little production . < br / > < br / ...  positive   \n",
            "2  thought wonderful way spend time hot summer we...  positive   \n",
            "3  Basically 's family little boy ( Jake ) think ...  negative   \n",
            "4  Petter Mattei 's `` Love Time Money '' visuall...  positive   \n",
            "\n",
            "  predicted_sentiment  \n",
            "0            positive  \n",
            "1            positive  \n",
            "2            positive  \n",
            "3            negative  \n",
            "4            positive  \n",
            "Predictions saved to 'sentiment_predictions.csv'\n"
          ]
        }
      ]
    }
  ]
}